{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is for ModelC\n",
    "build source model based on source countries data(CCPM and control measures) and predict CCPM for target countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T01:33:10.338223Z",
     "start_time": "2020-06-23T01:33:06.132786Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Part 1 - Data Preprocessing\n",
    "import tensorflow as tf\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import datetime \n",
    "\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import load_model,clone_model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense,  Lambda\n",
    "\n",
    "from keras.backend import slice\n",
    "from keras.constraints import max_norm\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='-1'\n",
    "\n",
    "##　locate the directory storing the data \n",
    "os.chdir(os.getcwd()+'/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T01:40:03.167003Z",
     "start_time": "2020-06-23T01:40:03.153967Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T01:40:03.672589Z",
     "start_time": "2020-06-23T01:40:03.664694Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_sequences_x(data, seq_length):\n",
    "    xs = []\n",
    "    for i in range(len(data)-seq_length+1):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        xs.append(x)\n",
    "    return np.array(xs)\n",
    "\n",
    "\n",
    "def create_sequences_y(data, seq_length):\n",
    "    ys = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        y = data[i]\n",
    "        ys.append(y)\n",
    "    return np.array(ys)\n",
    "\n",
    "def MAPE(y, y_pred):\n",
    "    mape = sum(abs(y-y_pred)/y)/len(y)\n",
    "    print('MAPE: ', mape)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造source序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T01:40:07.389239Z",
     "start_time": "2020-06-23T01:40:07.367544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Croatia',\n",
       "  'Norway',\n",
       "  'Lebanon',\n",
       "  'Hubei',\n",
       "  'Austria',\n",
       "  'China (except Hubei)',\n",
       "  'Oman',\n",
       "  'United Arab Emirates',\n",
       "  'Japan',\n",
       "  'Germany',\n",
       "  'Italy',\n",
       "  'Monaco'],\n",
       " 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from re import sub\n",
    "\n",
    "mypath = 'source/'\n",
    "source_countries = ['Austria','China (except Hubei)','Croatia','Germany','Hubei','Italy','Japan',\n",
    "            'Lebanon','Monaco','Norway','Oman','United Arab Emirates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T01:40:10.987217Z",
     "start_time": "2020-06-23T01:40:10.607456Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Croatia\n",
      "Norway\n",
      "Lebanon\n",
      "Hubei\n",
      "Austria\n",
      "China (except Hubei)\n",
      "Oman\n",
      "United Arab Emirates\n",
      "Japan\n",
      "Germany\n",
      "Italy\n",
      "Monaco\n"
     ]
    }
   ],
   "source": [
    "pred_length = 7\n",
    "seq_length = 7\n",
    "x_seq0 = []\n",
    "y_seq0 = []\n",
    "\n",
    "for sc in source_countries:\n",
    "    print(sc)\n",
    "    # data preprocessing\n",
    "    df = pd.read_excel('source/'+sc+'.xlsx', index_col=0)\n",
    "\n",
    "    df_new_1day = df[['confirmed cases per million']].diff(periods=1)\n",
    "    df_new_1day.rename(columns={'confirmed cases per million':'new cases'}, inplace=True)\n",
    "    df_new_1day['control'] = df.control.values\n",
    "\n",
    "    df_new_7days = df[['confirmed cases per million']].diff(periods=pred_length)\n",
    "    df_new_7days.rename(columns={'confirmed cases per million':'new cases'}, inplace=True)\n",
    "\n",
    "    df_new_7days['cum cases'] = 0\n",
    "    df_new_7days['cum cases'][pred_length:] = df['confirmed cases per million'].values[0:len(df_new_7days)-pred_length]  \n",
    "    \n",
    "    ## scale data\n",
    "    scaler_x = MinMaxScaler() #scale data into 0-1\n",
    "    scaler_y = MinMaxScaler() \n",
    "\n",
    "    if len(x_seq0)==0:\n",
    "        x_seq0 = create_sequences_x(df_new_1day.dropna().to_numpy(), seq_length)\n",
    "        y_seq0 = create_sequences_y(df_new_7days.dropna().to_numpy(), seq_length)\n",
    "        x_seq0 = x_seq0[0:len(y_seq0)]\n",
    "    else:\n",
    "        tx_seq0 = create_sequences_x(df_new_1day.dropna().to_numpy(), seq_length)\n",
    "        ty_seq0 = create_sequences_y(df_new_7days.dropna().to_numpy(), seq_length)\n",
    "        tx_seq0 = tx_seq0[0:len(ty_seq0)]\n",
    "        \n",
    "        x_seq0 = np.concatenate((x_seq0, tx_seq0),axis=0)\n",
    "        y_seq0 = np.concatenate((y_seq0, ty_seq0),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scale the sequence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T01:40:19.265411Z",
     "start_time": "2020-06-23T01:40:19.258660Z"
    }
   },
   "outputs": [],
   "source": [
    "x_seq1 = np.reshape(x_seq0, newshape=(-1,1))\n",
    "\n",
    "scaler_x = scaler_x.fit(x_seq1)\n",
    "x = scaler_x.transform(x_seq1)\n",
    "x = np.reshape(x, newshape=(x_seq0.shape))\n",
    "y_seq1 = np.reshape(y_seq0[:,0:1], newshape=(-1,1))\n",
    "\n",
    "scaler_y = scaler_y.fit(y_seq1)\n",
    "y = scaler_y.transform(y_seq1)\n",
    "\n",
    "y = np.reshape(y, newshape=(y_seq0[:,0:1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练source模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T01:40:22.158664Z",
     "start_time": "2020-06-23T01:40:22.155149Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T01:46:03.300217Z",
     "start_time": "2020-06-23T01:45:53.763048Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 593 samples\n",
      "Epoch 1/50\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0306 - val_loss: 0.0222\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 0s 392us/step - loss: 0.0254 - val_loss: 0.0196\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 0s 399us/step - loss: 0.0230 - val_loss: 0.0186\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 0s 376us/step - loss: 0.0221 - val_loss: 0.0184\n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 0s 373us/step - loss: 0.0218 - val_loss: 0.0183\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 0s 366us/step - loss: 0.0215 - val_loss: 0.0179\n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 0s 360us/step - loss: 0.0211 - val_loss: 0.0176\n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 0s 369us/step - loss: 0.0208 - val_loss: 0.0173\n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 0s 365us/step - loss: 0.0206 - val_loss: 0.0170\n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 0s 363us/step - loss: 0.0203 - val_loss: 0.0167\n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 0s 375us/step - loss: 0.0200 - val_loss: 0.0165\n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 0s 376us/step - loss: 0.0197 - val_loss: 0.0163\n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 0s 372us/step - loss: 0.0195 - val_loss: 0.0160\n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 0s 370us/step - loss: 0.0191 - val_loss: 0.0158\n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 0s 369us/step - loss: 0.0189 - val_loss: 0.0156\n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 0s 380us/step - loss: 0.0186 - val_loss: 0.0153\n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 0s 402us/step - loss: 0.0183 - val_loss: 0.0150\n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 0s 370us/step - loss: 0.0180 - val_loss: 0.0148\n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 0s 407us/step - loss: 0.0177 - val_loss: 0.0145\n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 0s 396us/step - loss: 0.0174 - val_loss: 0.0142\n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 0s 389us/step - loss: 0.0171 - val_loss: 0.0140\n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 0s 362us/step - loss: 0.0168 - val_loss: 0.0138\n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 0s 370us/step - loss: 0.0166 - val_loss: 0.0136\n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 0s 388us/step - loss: 0.0164 - val_loss: 0.0134\n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 0s 386us/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 0s 410us/step - loss: 0.0158 - val_loss: 0.0129\n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 0s 410us/step - loss: 0.0157 - val_loss: 0.0126\n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 0s 366us/step - loss: 0.0153 - val_loss: 0.0127\n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 0s 377us/step - loss: 0.0152 - val_loss: 0.0125\n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 0s 395us/step - loss: 0.0150 - val_loss: 0.0122\n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 0s 393us/step - loss: 0.0148 - val_loss: 0.0120\n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 0s 380us/step - loss: 0.0146 - val_loss: 0.0119\n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 0s 394us/step - loss: 0.0145 - val_loss: 0.0118\n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 0s 399us/step - loss: 0.0144 - val_loss: 0.0117\n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 0s 406us/step - loss: 0.0143 - val_loss: 0.0116\n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 0s 400us/step - loss: 0.0142 - val_loss: 0.0116\n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 0s 382us/step - loss: 0.0141 - val_loss: 0.0116\n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 0s 373us/step - loss: 0.0141 - val_loss: 0.0115\n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 0s 397us/step - loss: 0.0140 - val_loss: 0.0116\n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 0s 426us/step - loss: 0.0140 - val_loss: 0.0115\n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 0s 445us/step - loss: 0.0139 - val_loss: 0.0114\n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 0s 439us/step - loss: 0.0139 - val_loss: 0.0115\n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 0s 430us/step - loss: 0.0140 - val_loss: 0.0113\n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 0s 428us/step - loss: 0.0143 - val_loss: 0.0116\n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 0s 397us/step - loss: 0.0138 - val_loss: 0.0113\n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 0s 435us/step - loss: 0.0138 - val_loss: 0.0113\n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 0s 417us/step - loss: 0.0138 - val_loss: 0.0114\n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 0s 440us/step - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 0s 440us/step - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 0s 433us/step - loss: 0.0136 - val_loss: 0.0112\n",
      "MAPE:  1.309759287903574\n",
      "MAPE:  141.9056389988901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141.9056389988901"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed(123)\n",
    "train_idx = sample(range(len(x)),300)\n",
    "test_idx = set(range(len(x))).difference(train_idx)\n",
    "\n",
    "\n",
    "X_train = x[train_idx].copy()\n",
    "y_train = np.reshape(y[train_idx], (-1))\n",
    "\n",
    "X_test = x[list(test_idx)].copy()\n",
    "y_test = np.reshape(y[list(test_idx)], (-1))\n",
    "\n",
    "# Building the model\n",
    "main_input = Input(shape=(seq_length,2,), dtype='float32', name='main_input')  \n",
    "lstm_out = LSTM(4)(main_input)   \n",
    "main_output = Dense(units = 1)(lstm_out)\n",
    "regressor = Model(inputs=main_input, outputs=main_output)\n",
    "\n",
    "\n",
    "regressor.compile(optimizer='adam', loss='mse')\n",
    "regressor.fit(X_train, y_train, epochs = 50, batch_size = 32, validation_data=(X_test,y_test))\n",
    "regressor.save('../model/ModelC-control.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target countries - medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:14:37.024917Z",
     "start_time": "2020-06-23T06:14:37.008885Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from re import sub\n",
    "target_countries = ['Albania','Algeria','Argentina','Armenia','Australia','Azerbaijan','Bangladesh','Belarus','Belgium','Bermuda',\n",
    "             'Bolivia','Brazil','Bulgaria','Canada','Chile','Colombia','Costa Rica','Cuba','Czech Republic','Denmark',\n",
    "             'El Salvador','Estonia','Finland','France','Ghana','Gibraltar','Greece','Honduras','Hungary','India',\n",
    "             'Indonesia','Iran','Iraq','Ireland','Israel','Jamaica','Jordan','Liberia','Luxembourg','Malaysia','Mexico',\n",
    "             'Morocco','Nepal','Netherlands','Nigeria','Pakistan','Paraguay','Peru','Philippines','Poland','Portugal',\n",
    "             'Qatar','Republic of the Congo','Romania','Russia','Rwanda','Saudi Arabia','Senegal','Sierra Leone',\n",
    "             'Singapore','Slovakia','Slovenia','South Africa','Sri Lanka','Switzerland','Thailand','Tunisia','Turkey',\n",
    "             'Ukraine','United Kingdom','United States','Venezuela']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T09:20:39.831197Z",
     "start_time": "2020-06-19T09:20:39.815008Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_model = load_model('../model/ModelC-control.pkl')\n",
    "ret_test = pd.DataFrame(index=target_countries, columns=['MAPE'])\n",
    "modelc_control = pd.DataFrame(index=['loop1', 'loop2', 'loop3', 'loop4','loop5'], columns=['MAPE(mean)','MAPE(std)','MAPE<0.1','MAPE<0.05'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T10:05:12.309631Z",
     "start_time": "2020-06-19T09:34:13.661553Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iraq\n",
      "MAPE:  0.1010970806296633\n",
      "Denmark\n",
      "MAPE:  0.03745080750703928\n",
      "Republic of the Congo\n",
      "MAPE:  0.03700844144024166\n",
      "Slovakia\n",
      "MAPE:  0.04057251380286587\n",
      "Azerbaijan\n",
      "MAPE:  0.06491090043406075\n",
      "Australia\n",
      "MAPE:  0.04540069997822493\n",
      "Bolivia\n",
      "MAPE:  0.16201956809349902\n",
      "Bangladesh\n",
      "MAPE:  0.10171506357672477\n",
      "Indonesia\n",
      "MAPE:  0.04695581045680596\n",
      "Estonia\n",
      "MAPE:  0.03193089629341609\n",
      "Rwanda\n",
      "MAPE:  0.09133714841478137\n",
      "Nigeria\n",
      "MAPE:  0.06854949295051346\n",
      "Bermuda\n",
      "MAPE:  0.030609473925073495\n",
      "Jordan\n",
      "MAPE:  0.039542500962664644\n",
      "Argentina\n",
      "MAPE:  0.15328290571695996\n",
      "Finland\n",
      "MAPE:  0.047290475889908534\n",
      "Paraguay\n",
      "MAPE:  0.07748345325170151\n",
      "Saudi Arabia\n",
      "MAPE:  0.05200179923381\n",
      "Turkey\n",
      "MAPE:  0.039656551317122375\n",
      "Ireland\n",
      "MAPE:  0.04031761899627622\n",
      "Iran\n",
      "MAPE:  0.015912292409801902\n",
      "Switzerland\n",
      "MAPE:  0.03872265756354139\n",
      "Colombia\n",
      "MAPE:  0.08940600138766834\n",
      "Albania\n",
      "MAPE:  0.023755701435803786\n",
      "Pakistan\n",
      "MAPE:  0.06056629996624596\n",
      "Belgium\n",
      "MAPE:  0.03445783736177821\n",
      "United States\n",
      "MAPE:  0.016500321648057793\n",
      "Israel\n",
      "MAPE:  0.0458620152694296\n",
      "Liberia\n",
      "MAPE:  0.03674790457571623\n",
      "Philippines\n",
      "MAPE:  0.03219619088614057\n",
      "Morocco\n",
      "MAPE:  0.05027485401696619\n",
      "Nepal\n",
      "MAPE:  0.3031328306125513\n",
      "Ghana\n",
      "MAPE:  0.04589940685673189\n",
      "Poland\n",
      "MAPE:  0.009685965962041639\n",
      "Mexico\n",
      "MAPE:  0.07854468976339568\n",
      "Tunisia\n",
      "MAPE:  0.019441828674510588\n",
      "Peru\n",
      "MAPE:  0.05710948690810883\n",
      "Belarus\n",
      "MAPE:  0.006992418911910172\n",
      "Czech Republic\n",
      "MAPE:  0.023816206784531065\n",
      "Gibraltar\n",
      "MAPE:  0.022340773878437976\n",
      "Brazil\n",
      "MAPE:  0.10956421663263753\n",
      "Qatar\n",
      "MAPE:  0.132323671522127\n",
      "South Africa\n",
      "MAPE:  0.12047639325749442\n",
      "Cuba\n",
      "MAPE:  0.040103528316348\n",
      "Jamaica\n",
      "MAPE:  0.018324695705390574\n",
      "Bulgaria\n",
      "MAPE:  0.03390538486682096\n",
      "Honduras\n",
      "MAPE:  0.13101468578927078\n",
      "Netherlands\n",
      "MAPE:  0.03837586291258177\n",
      "Sierra Leone\n",
      "MAPE:  0.04782134094961249\n",
      "Armenia\n",
      "MAPE:  0.1464775503549638\n",
      "Venezuela\n",
      "MAPE:  0.30777972899916034\n",
      "Slovenia\n",
      "MAPE:  0.039850134307502894\n",
      "India\n",
      "MAPE:  0.10158696652049141\n",
      "Malaysia\n",
      "MAPE:  0.028100441649072914\n",
      "Ukraine\n",
      "MAPE:  0.01592217161661723\n",
      "Portugal\n",
      "MAPE:  0.03034796702018238\n",
      "Thailand\n",
      "MAPE:  0.03453757548336052\n",
      "Hungary\n",
      "MAPE:  0.03218081359975864\n",
      "Luxembourg\n",
      "MAPE:  0.028978297352715485\n",
      "Sri Lanka\n",
      "MAPE:  0.12681404329288415\n",
      "Russia\n",
      "MAPE:  0.02020613429385411\n",
      "France\n",
      "MAPE:  0.031215048536006814\n",
      "Romania\n",
      "MAPE:  0.03446449070884151\n",
      "Singapore\n",
      "MAPE:  0.025097902680645035\n",
      "Algeria\n",
      "MAPE:  0.013029685422412285\n",
      "Georgia\n",
      "MAPE:  0.03831619080348778\n",
      "Costa Rica\n",
      "MAPE:  0.01528882016779439\n",
      "Greece\n",
      "MAPE:  0.03561320432476846\n",
      "Canada\n",
      "MAPE:  0.01959064789884628\n",
      "United Kingdom\n",
      "MAPE:  0.03539797058085577\n",
      "Chile\n",
      "MAPE:  0.15675470499305183\n",
      "El Salvador\n",
      "MAPE:  0.061896475002123044\n",
      "Senegal\n",
      "MAPE:  0.00868431583221342\n"
     ]
    }
   ],
   "source": [
    "mypath = 'target/'\n",
    "for l in range(5):\n",
    "    for tar in target_countries:\n",
    "        print(tar)\n",
    "        # data preprocessing\n",
    "        df = pd.read_excel(mypath+tar+'.xlsx', index_col=0)\n",
    "\n",
    "        df_new_1day = df.diff(periods=1)\n",
    "        df_new_1day.rename(columns={'confirmed cases per million':'new cases'}, inplace=True)\n",
    "\n",
    "        df_new_1day['control'] = df.control.values\n",
    "\n",
    "        df_new_7days = df[['confirmed cases per million']].diff(periods=pred_length)\n",
    "        df_new_7days.rename(columns={'confirmed cases per million':'new cases'}, inplace=True)\n",
    "\n",
    "        df_new_7days['cum cases'] = 0\n",
    "        df_new_7days['cum cases'][pred_length:] = df['confirmed cases per million'].values[0:len(df_new_7days)-pred_length]\n",
    "\n",
    "        x_seq0 = create_sequences_x(df_new_1day.dropna().to_numpy(), seq_length)\n",
    "        y_seq0 = create_sequences_y(df_new_7days.dropna().to_numpy(), seq_length)\n",
    "        x_seq0 = x_seq0[0:len(y_seq0)]\n",
    "\n",
    "        x_seq1 = np.reshape(x_seq0, newshape=(-1,1))\n",
    "        x = scaler_x.transform(x_seq1)\n",
    "\n",
    "        x = np.reshape(x, newshape=(x_seq0.shape))\n",
    "\n",
    "        y_seq1 = np.reshape(y_seq0[:,0:1], newshape=(-1,1))\n",
    "        y = scaler_y.transform(y_seq1)\n",
    "\n",
    "        y = np.reshape(y, newshape=(y_seq0[:,0:1].shape))\n",
    "\n",
    "        test_idx = int(len(x)*0.8)\n",
    "        X_train = x[0:test_idx, :, 0:2].copy()\n",
    "        y_train = np.reshape(y[0:test_idx], (-1))\n",
    "\n",
    "        X_test = x[test_idx:,:,0:2].copy()\n",
    "        y_test = np.reshape(y[test_idx:], (-1))\n",
    "\n",
    "        model =clone_model(pred_model)\n",
    "        model.set_weights(pred_model.get_weights())\n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.trainable=False\n",
    "        for layer in model.layers[-1:]:\n",
    "            layer.trainable=True\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        # Fitting the RNN to the Training set\n",
    "        model.fit(X_train, y_train, epochs = 12, batch_size = 2, verbose=0)\n",
    "\n",
    "        # Predicting daily cases\n",
    "        predicted_cases = model.predict(X_test)\n",
    "        predicted_cases = scaler_y.inverse_transform(predicted_cases)\n",
    "\n",
    "        true_cases = np.reshape(scaler_y.inverse_transform(np.reshape(y_test,(-1,1))),(-1))+y_seq0[test_idx:,1]\n",
    "        predicted_cases = np.reshape(predicted_cases,(-1))+y_seq0[test_idx:,1]\n",
    "\n",
    "\n",
    "        mape = MAPE(true_cases, predicted_cases) \n",
    "        ret_test.loc[tar, 'MAPE'] = mape\n",
    "    modelc_control.loc['loop'+str(l+1), 'MAPE(mean)'] = ret_test.MAPE.mean() \n",
    "    modelc_control.loc['loop'+str(l+1), 'MAPE(std)'] = ret_test.MAPE.std()\n",
    "    modelc_control.loc['loop'+str(l+1), 'MAPE<0.1'] = ret_test[(ret_test.MAPE<0.1)].shape[0]\n",
    "    modelc_control.loc['loop'+str(l+1), 'MAPE<0.05']=ret_test[(ret_test.MAPE<0.05)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T01:52:42.133596Z",
     "start_time": "2020-06-23T01:52:42.123494Z"
    }
   },
   "outputs": [],
   "source": [
    "modelc_control.to_csv('../result/modelC_control.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
