{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:38:19.481334Z",
     "start_time": "2020-06-05T07:38:16.366759Z"
    }
   },
   "source": [
    "# The code is used to build the pre_train model for the source countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T04:53:56.038497Z",
     "start_time": "2020-06-23T04:53:52.709531Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import time \n",
    "import keras \n",
    "import sys\n",
    "import random\n",
    "\n",
    "\n",
    "from keras.layers import multiply\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "import keras.backend as K\n",
    "from keras.layers import Concatenate\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "os.getcwd()\n",
    "seed=666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T04:54:00.310786Z",
     "start_time": "2020-06-23T04:54:00.298952Z"
    }
   },
   "outputs": [],
   "source": [
    "#create the sequence slice for the model\n",
    "def create_sequences(data, seq_length, next_days):\n",
    "\n",
    "    N=len(data)-seq_length-next_days+1\n",
    "    xs = np.zeros((N,seq_length,1))   \n",
    "    ys = np.zeros((N,1))\n",
    "    cs = np.zeros((N,seq_length,1))\n",
    "    \n",
    "    for i in range(N):\n",
    "        xs[i,:,0] = data[i:i+seq_length,0]\n",
    "        ys[i] = data[i+seq_length:i+seq_length+next_days,0].sum() \n",
    "        cs[i,:,0] = data[i:i+seq_length,1]\n",
    "    return xs,ys,cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T04:54:01.176525Z",
     "start_time": "2020-06-23T04:54:01.162334Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_attention_applied_after_lstm(x_input,c_input,y_input):\n",
    "    INPUT_DIM =1\n",
    "    TIME_STEPS=7\n",
    "    lstm_units = 32\n",
    "\n",
    "    inputs1 = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    inputs2 = Input(shape=(TIME_STEPS,))  \n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs1)\n",
    "\n",
    "    a = Permute((2, 1))(lstm_out)\n",
    "    a = Reshape((lstm_units, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "    a = RepeatVector(INPUT_DIM)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([lstm_out, a_probs])\n",
    "    attention_mul = Flatten()(output_attention_mul) \n",
    "    newcase_output = Dense(16)(attention_mul)        \n",
    "    newcase_output = Dense(1,activation='sigmoid')(newcase_output)\n",
    "\n",
    "   #add the weight to the lockdown measures according to the domain knowledge\n",
    "    def control_rate(x,rate=0.1):\n",
    "        rate2= rate*K.ones((7,1))\n",
    "        return  K.dot(x,rate2)\n",
    "\n",
    "    inputs1_new = Flatten()(inputs1)    \n",
    "    inputs2_new = multiply([inputs2, inputs1_new])\n",
    "    control_output=Lambda(control_rate)(inputs2_new) \n",
    "   \n",
    "    output=keras.layers.Add()([newcase_output,control_output])\n",
    "\n",
    "\n",
    "    model = Model(input=[inputs1 ,inputs2], output=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit([x_input,c_input], y_input, epochs=100, batch_size=32,  \n",
    "              validation_data=([x_input,c_input],y_input))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T03:03:06.396031Z",
     "start_time": "2020-06-23T03:02:07.395075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data1/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"ad...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data1/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 903 samples, validate on 903 samples\n",
      "Epoch 1/100\n",
      "903/903 [==============================] - 2s 2ms/step - loss: 0.1335 - val_loss: 0.1060\n",
      "Epoch 2/100\n",
      "903/903 [==============================] - 1s 579us/step - loss: 0.0802 - val_loss: 0.0717\n",
      "Epoch 3/100\n",
      "903/903 [==============================] - 1s 572us/step - loss: 0.0668 - val_loss: 0.0625\n",
      "Epoch 4/100\n",
      "903/903 [==============================] - 1s 576us/step - loss: 0.0598 - val_loss: 0.0567\n",
      "Epoch 5/100\n",
      "903/903 [==============================] - 1s 568us/step - loss: 0.0547 - val_loss: 0.0534\n",
      "Epoch 6/100\n",
      "903/903 [==============================] - 1s 560us/step - loss: 0.0525 - val_loss: 0.0517\n",
      "Epoch 7/100\n",
      "903/903 [==============================] - 1s 567us/step - loss: 0.0516 - val_loss: 0.0507\n",
      "Epoch 8/100\n",
      "903/903 [==============================] - 1s 569us/step - loss: 0.0504 - val_loss: 0.0505\n",
      "Epoch 9/100\n",
      "903/903 [==============================] - 1s 570us/step - loss: 0.0499 - val_loss: 0.0490\n",
      "Epoch 10/100\n",
      "903/903 [==============================] - 1s 561us/step - loss: 0.0491 - val_loss: 0.0485\n",
      "Epoch 11/100\n",
      "903/903 [==============================] - 1s 580us/step - loss: 0.0484 - val_loss: 0.0475\n",
      "Epoch 12/100\n",
      "903/903 [==============================] - 1s 570us/step - loss: 0.0473 - val_loss: 0.0469\n",
      "Epoch 13/100\n",
      "903/903 [==============================] - 1s 561us/step - loss: 0.0467 - val_loss: 0.0462\n",
      "Epoch 14/100\n",
      "903/903 [==============================] - 1s 558us/step - loss: 0.0459 - val_loss: 0.0456\n",
      "Epoch 15/100\n",
      "903/903 [==============================] - 1s 574us/step - loss: 0.0452 - val_loss: 0.0460\n",
      "Epoch 16/100\n",
      "903/903 [==============================] - 1s 567us/step - loss: 0.0446 - val_loss: 0.0442\n",
      "Epoch 17/100\n",
      "903/903 [==============================] - 1s 570us/step - loss: 0.0441 - val_loss: 0.0441\n",
      "Epoch 18/100\n",
      "903/903 [==============================] - 1s 580us/step - loss: 0.0434 - val_loss: 0.0432\n",
      "Epoch 19/100\n",
      "903/903 [==============================] - 1s 580us/step - loss: 0.0431 - val_loss: 0.0424\n",
      "Epoch 20/100\n",
      "903/903 [==============================] - 1s 576us/step - loss: 0.0427 - val_loss: 0.0420\n",
      "Epoch 21/100\n",
      "903/903 [==============================] - 1s 577us/step - loss: 0.0422 - val_loss: 0.0424\n",
      "Epoch 22/100\n",
      "903/903 [==============================] - 1s 584us/step - loss: 0.0421 - val_loss: 0.0406\n",
      "Epoch 23/100\n",
      "903/903 [==============================] - 1s 574us/step - loss: 0.0417 - val_loss: 0.0406\n",
      "Epoch 24/100\n",
      "903/903 [==============================] - 1s 580us/step - loss: 0.0408 - val_loss: 0.0402\n",
      "Epoch 25/100\n",
      "903/903 [==============================] - 1s 561us/step - loss: 0.0400 - val_loss: 0.0394\n",
      "Epoch 26/100\n",
      "903/903 [==============================] - 0s 550us/step - loss: 0.0390 - val_loss: 0.0387\n",
      "Epoch 27/100\n",
      "903/903 [==============================] - 0s 553us/step - loss: 0.0389 - val_loss: 0.0384\n",
      "Epoch 28/100\n",
      "903/903 [==============================] - 0s 542us/step - loss: 0.0382 - val_loss: 0.0381\n",
      "Epoch 29/100\n",
      "903/903 [==============================] - 0s 545us/step - loss: 0.0386 - val_loss: 0.0378\n",
      "Epoch 30/100\n",
      "903/903 [==============================] - 0s 537us/step - loss: 0.0390 - val_loss: 0.0391\n",
      "Epoch 31/100\n",
      "903/903 [==============================] - 0s 532us/step - loss: 0.0385 - val_loss: 0.0369\n",
      "Epoch 32/100\n",
      "903/903 [==============================] - 0s 546us/step - loss: 0.0371 - val_loss: 0.0366\n",
      "Epoch 33/100\n",
      "903/903 [==============================] - 0s 535us/step - loss: 0.0371 - val_loss: 0.0365\n",
      "Epoch 34/100\n",
      "903/903 [==============================] - 0s 539us/step - loss: 0.0370 - val_loss: 0.0364\n",
      "Epoch 35/100\n",
      "903/903 [==============================] - 0s 534us/step - loss: 0.0372 - val_loss: 0.0361\n",
      "Epoch 36/100\n",
      "903/903 [==============================] - 0s 538us/step - loss: 0.0366 - val_loss: 0.0384\n",
      "Epoch 37/100\n",
      "903/903 [==============================] - 1s 573us/step - loss: 0.0382 - val_loss: 0.0360\n",
      "Epoch 38/100\n",
      "903/903 [==============================] - 1s 575us/step - loss: 0.0395 - val_loss: 0.0370\n",
      "Epoch 39/100\n",
      "903/903 [==============================] - 1s 569us/step - loss: 0.0367 - val_loss: 0.0362\n",
      "Epoch 40/100\n",
      "903/903 [==============================] - 1s 559us/step - loss: 0.0364 - val_loss: 0.0375\n",
      "Epoch 41/100\n",
      "903/903 [==============================] - 0s 552us/step - loss: 0.0365 - val_loss: 0.0362\n",
      "Epoch 42/100\n",
      "903/903 [==============================] - 0s 545us/step - loss: 0.0362 - val_loss: 0.0356\n",
      "Epoch 43/100\n",
      "903/903 [==============================] - 0s 552us/step - loss: 0.0375 - val_loss: 0.0364\n",
      "Epoch 44/100\n",
      "903/903 [==============================] - 1s 564us/step - loss: 0.0377 - val_loss: 0.0358\n",
      "Epoch 45/100\n",
      "903/903 [==============================] - 1s 562us/step - loss: 0.0363 - val_loss: 0.0356\n",
      "Epoch 46/100\n",
      "903/903 [==============================] - 1s 557us/step - loss: 0.0357 - val_loss: 0.0357\n",
      "Epoch 47/100\n",
      "903/903 [==============================] - 1s 566us/step - loss: 0.0356 - val_loss: 0.0353\n",
      "Epoch 48/100\n",
      "903/903 [==============================] - 1s 568us/step - loss: 0.0359 - val_loss: 0.0352\n",
      "Epoch 49/100\n",
      "903/903 [==============================] - 1s 573us/step - loss: 0.0360 - val_loss: 0.0354\n",
      "Epoch 50/100\n",
      "903/903 [==============================] - 1s 569us/step - loss: 0.0359 - val_loss: 0.0355\n",
      "Epoch 51/100\n",
      "903/903 [==============================] - 1s 575us/step - loss: 0.0355 - val_loss: 0.0358\n",
      "Epoch 52/100\n",
      "903/903 [==============================] - 1s 576us/step - loss: 0.0353 - val_loss: 0.0350\n",
      "Epoch 53/100\n",
      "903/903 [==============================] - 1s 582us/step - loss: 0.0361 - val_loss: 0.0352\n",
      "Epoch 54/100\n",
      "903/903 [==============================] - 1s 606us/step - loss: 0.0352 - val_loss: 0.0354\n",
      "Epoch 55/100\n",
      "903/903 [==============================] - 1s 577us/step - loss: 0.0354 - val_loss: 0.0350\n",
      "Epoch 56/100\n",
      "903/903 [==============================] - 1s 627us/step - loss: 0.0351 - val_loss: 0.0349\n",
      "Epoch 57/100\n",
      "903/903 [==============================] - 1s 611us/step - loss: 0.0356 - val_loss: 0.0348\n",
      "Epoch 58/100\n",
      "903/903 [==============================] - 1s 634us/step - loss: 0.0355 - val_loss: 0.0350\n",
      "Epoch 59/100\n",
      "903/903 [==============================] - 1s 646us/step - loss: 0.0354 - val_loss: 0.0348\n",
      "Epoch 60/100\n",
      "903/903 [==============================] - 1s 618us/step - loss: 0.0350 - val_loss: 0.0346\n",
      "Epoch 61/100\n",
      "903/903 [==============================] - 1s 598us/step - loss: 0.0346 - val_loss: 0.0357\n",
      "Epoch 62/100\n",
      "903/903 [==============================] - 1s 601us/step - loss: 0.0353 - val_loss: 0.0353\n",
      "Epoch 63/100\n",
      "903/903 [==============================] - 1s 606us/step - loss: 0.0361 - val_loss: 0.0357\n",
      "Epoch 64/100\n",
      "903/903 [==============================] - 1s 560us/step - loss: 0.0350 - val_loss: 0.0348\n",
      "Epoch 65/100\n",
      "903/903 [==============================] - 1s 558us/step - loss: 0.0351 - val_loss: 0.0359\n",
      "Epoch 66/100\n",
      "903/903 [==============================] - 1s 568us/step - loss: 0.0355 - val_loss: 0.0346\n",
      "Epoch 67/100\n",
      "903/903 [==============================] - 1s 583us/step - loss: 0.0351 - val_loss: 0.0368\n",
      "Epoch 68/100\n",
      "903/903 [==============================] - 1s 589us/step - loss: 0.0351 - val_loss: 0.0344\n",
      "Epoch 69/100\n",
      "903/903 [==============================] - 1s 581us/step - loss: 0.0351 - val_loss: 0.0354\n",
      "Epoch 70/100\n",
      "903/903 [==============================] - 1s 574us/step - loss: 0.0355 - val_loss: 0.0347\n",
      "Epoch 71/100\n",
      "903/903 [==============================] - 1s 587us/step - loss: 0.0348 - val_loss: 0.0345\n",
      "Epoch 72/100\n",
      "903/903 [==============================] - 1s 579us/step - loss: 0.0346 - val_loss: 0.0347\n",
      "Epoch 73/100\n",
      "903/903 [==============================] - 1s 576us/step - loss: 0.0351 - val_loss: 0.0343\n",
      "Epoch 74/100\n",
      "903/903 [==============================] - 1s 570us/step - loss: 0.0354 - val_loss: 0.0342\n",
      "Epoch 75/100\n",
      "903/903 [==============================] - 1s 571us/step - loss: 0.0345 - val_loss: 0.0343\n",
      "Epoch 76/100\n",
      "903/903 [==============================] - 1s 579us/step - loss: 0.0347 - val_loss: 0.0341\n",
      "Epoch 77/100\n",
      "903/903 [==============================] - 1s 580us/step - loss: 0.0345 - val_loss: 0.0351\n",
      "Epoch 78/100\n",
      "903/903 [==============================] - 1s 579us/step - loss: 0.0350 - val_loss: 0.0343\n",
      "Epoch 79/100\n",
      "903/903 [==============================] - 1s 593us/step - loss: 0.0347 - val_loss: 0.0343\n",
      "Epoch 80/100\n",
      "903/903 [==============================] - 1s 578us/step - loss: 0.0352 - val_loss: 0.0342\n",
      "Epoch 81/100\n",
      "903/903 [==============================] - 1s 585us/step - loss: 0.0346 - val_loss: 0.0341\n",
      "Epoch 82/100\n",
      "903/903 [==============================] - 1s 576us/step - loss: 0.0346 - val_loss: 0.0340\n",
      "Epoch 83/100\n",
      "903/903 [==============================] - 1s 576us/step - loss: 0.0351 - val_loss: 0.0341\n",
      "Epoch 84/100\n",
      "903/903 [==============================] - 1s 602us/step - loss: 0.0341 - val_loss: 0.0342\n",
      "Epoch 85/100\n",
      "903/903 [==============================] - 1s 571us/step - loss: 0.0343 - val_loss: 0.0343\n",
      "Epoch 86/100\n",
      "903/903 [==============================] - 1s 557us/step - loss: 0.0343 - val_loss: 0.0341\n",
      "Epoch 87/100\n",
      "903/903 [==============================] - 1s 570us/step - loss: 0.0345 - val_loss: 0.0338\n",
      "Epoch 88/100\n",
      "903/903 [==============================] - 1s 570us/step - loss: 0.0343 - val_loss: 0.0338\n",
      "Epoch 89/100\n",
      "903/903 [==============================] - 1s 580us/step - loss: 0.0342 - val_loss: 0.0338\n",
      "Epoch 90/100\n",
      "903/903 [==============================] - 1s 569us/step - loss: 0.0343 - val_loss: 0.0338\n",
      "Epoch 91/100\n",
      "903/903 [==============================] - 1s 567us/step - loss: 0.0340 - val_loss: 0.0336\n",
      "Epoch 92/100\n",
      "903/903 [==============================] - 1s 583us/step - loss: 0.0339 - val_loss: 0.0352\n",
      "Epoch 93/100\n",
      "903/903 [==============================] - 1s 560us/step - loss: 0.0353 - val_loss: 0.0345\n",
      "Epoch 94/100\n",
      "903/903 [==============================] - 1s 580us/step - loss: 0.0339 - val_loss: 0.0336\n",
      "Epoch 95/100\n",
      "903/903 [==============================] - 1s 579us/step - loss: 0.0340 - val_loss: 0.0336\n",
      "Epoch 96/100\n",
      "903/903 [==============================] - 1s 590us/step - loss: 0.0338 - val_loss: 0.0337\n",
      "Epoch 97/100\n",
      "903/903 [==============================] - 1s 560us/step - loss: 0.0341 - val_loss: 0.0336\n",
      "Epoch 98/100\n",
      "903/903 [==============================] - 1s 571us/step - loss: 0.0338 - val_loss: 0.0340\n",
      "Epoch 99/100\n",
      "903/903 [==============================] - 1s 607us/step - loss: 0.0347 - val_loss: 0.0353\n",
      "Epoch 100/100\n",
      "903/903 [==============================] - 1s 620us/step - loss: 0.0341 - val_loss: 0.0335\n"
     ]
    }
   ],
   "source": [
    "#list the source countries\n",
    "source_countries=['Austria','China (except Hubei)','Croatia','Germany','Hubei','Italy','Japan',\n",
    "            'Lebanon','Monaco','Norway','Oman','United Arab Emirates']\n",
    "\n",
    "seq_length=7\n",
    "next_days=7\n",
    "\n",
    "x_train_total=np.zeros((0,seq_length,1))\n",
    "c_train_total=np.zeros((0,seq_length))\n",
    "y_train_total=np.zeros((0,1))\n",
    "\n",
    "INPUT_DIM = 2\n",
    "TIME_STEPS = 7\n",
    "\n",
    "for i in range(len(source_countries)):\n",
    "    #load the data \n",
    "    country_name='./data/source/'+source_countries[i]+'.xlsx'\n",
    "    data_dist =  pd.read_excel(country_name,encoding='gbk').dropna()\n",
    "    data_dist=data_dist.drop(['截止时间'],axis=1)\n",
    "    data_diff= data_dist['confirmed cases per million'].diff().dropna()\n",
    "    data_diff=pd.concat([data_diff,data_dist.iloc[1:,1]],axis=1)\n",
    "    data_diff=np.array( data_diff)\n",
    "    \n",
    "    x,y,c=create_sequences(data_diff, seq_length, next_days)\n",
    "    \n",
    "    #normalization\n",
    "    x_max=x.max()\n",
    "    x_min=x.min()\n",
    "    x=(x-x_min)/(x_max-x_min)\n",
    "    \n",
    "    y_max=y.max()\n",
    "    y_min=y.min()\n",
    "    y=(y-y_min)/(y_max-y_min)\n",
    "\n",
    "    # concat all the countries samples to train a source model\n",
    "    c=np.ones((c.shape[0],c.shape[1]))-c.reshape(-1,seq_length)\n",
    "    \n",
    "    x_train=x.reshape(-1,seq_length,1)\n",
    "    c_train=c.reshape(-1,seq_length)\n",
    "    y_train=y.reshape(-1,1)    \n",
    "    \n",
    "    x_train_total=np.concatenate((x_train_total,x_train),axis=0)\n",
    "    c_train_total=np.concatenate((c_train_total,c_train),axis=0)    \n",
    "    y_train_total=np.concatenate((y_train_total,y_train),axis=0)    \n",
    "\n",
    "model = model_attention_applied_after_lstm(x_train_total,c_train_total,y_train_total)\n",
    "#save the model\n",
    "model.save('./model/source_model_ALerT-COVID.pkl')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code is used to verify the effect of ALerT-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T04:29:33.192609Z",
     "start_time": "2020-06-23T03:04:06.025748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Albania\n",
      "1 Algeria\n",
      "2 Argentina\n",
      "3 Armenia\n",
      "4 Australia\n",
      "5 Azerbaijan\n",
      "6 Bangladesh\n",
      "7 Belarus\n",
      "8 Belgium\n",
      "9 Bermuda\n",
      "10 Bolivia\n",
      "11 Brazil\n",
      "12 Bulgaria\n",
      "13 Canada\n",
      "14 Chile\n",
      "15 Colombia\n",
      "16 Costa Rica\n",
      "17 Cuba\n",
      "18 Czech Republic\n",
      "19 Denmark\n",
      "20 El Salvador\n",
      "21 Estonia\n",
      "22 Finland\n",
      "23 France\n",
      "24 Ghana\n",
      "25 Gibraltar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:85: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 Greece\n",
      "27 Honduras\n",
      "28 Hungary\n",
      "29 India\n",
      "30 Indonesia\n",
      "31 Iran\n",
      "32 Iraq\n",
      "33 Ireland\n",
      "34 Israel\n",
      "35 Jamaica\n",
      "36 Jordan\n",
      "37 Liberia\n",
      "38 Luxembourg\n",
      "39 Malaysia\n",
      "40 Mexico\n",
      "41 Morocco\n",
      "42 Nepal\n",
      "43 Netherlands\n",
      "44 Nigeria\n",
      "45 Pakistan\n",
      "46 Paraguay\n",
      "47 Peru\n",
      "48 Philippines\n",
      "49 Poland\n",
      "50 Portugal\n",
      "51 Qatar\n",
      "52 Republic of the Congo\n",
      "53 Romania\n",
      "54 Russia\n",
      "55 Rwanda\n",
      "56 Saudi Arabia\n",
      "57 Senegal\n",
      "58 Sierra Leone\n",
      "59 Singapore\n",
      "60 Slovakia\n",
      "61 Slovenia\n",
      "62 South Africa\n",
      "63 Sri Lanka\n",
      "64 Switzerland\n",
      "65 Thailand\n",
      "66 Tunisia\n",
      "67 Turkey\n",
      "68 Ukraine\n",
      "69 United Kingdom\n",
      "70 United States\n",
      "71 Venezuela\n"
     ]
    }
   ],
   "source": [
    "# list the name of target countries\n",
    "target_cols=['Albania','Algeria','Argentina','Armenia','Australia','Azerbaijan','Bangladesh','Belarus','Belgium','Bermuda',\n",
    "             'Bolivia','Brazil','Bulgaria','Canada','Chile','Colombia','Costa Rica','Cuba','Czech Republic','Denmark',\n",
    "             'El Salvador','Estonia','Finland','France','Ghana','Gibraltar','Greece','Honduras','Hungary','India',\n",
    "             'Indonesia','Iran','Iraq','Ireland','Israel','Jamaica','Jordan','Liberia','Luxembourg','Malaysia','Mexico',\n",
    "             'Morocco','Nepal','Netherlands','Nigeria','Pakistan','Paraguay','Peru','Philippines','Poland','Portugal',\n",
    "             'Qatar','Republic of the Congo','Romania','Russia','Rwanda','Saudi Arabia','Senegal','Sierra Leone',\n",
    "             'Singapore','Slovakia','Slovenia','South Africa','Sri Lanka','Switzerland','Thailand','Tunisia','Turkey',\n",
    "             'Ukraine','United Kingdom','United States','Venezuela']\n",
    "\n",
    "#set the parameter\n",
    "seq_length=7\n",
    "next_days=7\n",
    "\n",
    "\n",
    "pred_total=[]\n",
    "true_data_total=[]\n",
    "pred_data_total=[]\n",
    "new_case_mape_total=[]\n",
    "cumulative_case_mape_total=[]\n",
    "\n",
    "for i in range(len(target_cols)):\n",
    "    print(i,target_cols[i])\n",
    "    # load the data\n",
    "    country_name='./data/target/'+target_cols[i]+'.xlsx'\n",
    "    data_dist =  pd.read_excel(country_name,encoding='gbk').dropna()\n",
    "    data_dist=data_dist.drop(['截止时间'],axis=1)\n",
    "    data_diff= data_dist['confirmed cases per million'].diff().dropna()\n",
    "    data_diff=pd.concat([data_diff,data_dist.iloc[1:,1]],axis=1)\n",
    "    data_diff=np.array(data_diff)\n",
    "    \n",
    "    x,y,c=create_sequences(data_diff, seq_length, next_days)\n",
    "    \n",
    "    # normalization \n",
    "    c=np.ones((c.shape[0],c.shape[1]))-c.reshape(-1,seq_length)\n",
    "    x_max=x.max()\n",
    "    x_min=x.min()\n",
    "    x=(x-x_min)/(x_max-x_min)\n",
    "    \n",
    "    y_max=y.max()\n",
    "    y_min=y.min()\n",
    "    y=(y-y_min)/(y_max-y_min)\n",
    "\n",
    "    #split the target countries data, 80% of the data is used to fine tune source model,20% is used to verify the effect  \n",
    "    index1=int(len(x)*0.8)\n",
    "    \n",
    "    x_train=x[:index1].reshape(-1,seq_length,1)\n",
    "    c_train=c[:index1].reshape(-1,seq_length)\n",
    "    y_train=y[:index1].reshape(-1,1)\n",
    "\n",
    "    \n",
    "    x_val=x[index1:].reshape(-1,seq_length,1)\n",
    "    c_val=c[index1:].reshape(-1,seq_length)\n",
    "    y_val=y[index1:].reshape(-1,1)\n",
    "    \n",
    "    #load the source model and fine_tune the model   \n",
    "    source_model='./model/source_model_ALerT-COVID.pkl'  \n",
    "    pre_model= load_model(source_model) \n",
    "    \n",
    "    for layer in pre_model.layers[:len(pre_model.layers)-2]:\n",
    "        layer.trainable = False \n",
    "        \n",
    "    pre_model.compile(loss='mse', optimizer=Adam(lr=1e-3))   \n",
    "    pre_model.fit([x_train,c_train], y_train,epochs=15, batch_size=4,\n",
    "                  validation_data=([x_val,c_val],y_val),verbose = 0)\n",
    " \n",
    "\n",
    "    pred_val = pre_model.predict([x_val,c_val])[-1,0]\n",
    "    pred_val = np.array(pred_val)*(y_max-y_min)+y_min \n",
    "    pred_val = np.array(data_dist.iloc[-8,0])+pred_val\n",
    "    \n",
    "    \n",
    "    true_val = y_val[-1,0]\n",
    "    true_val = np.array(true_val)*(y_max-y_min)+y_min \n",
    "    true_val = np.array(data_dist.iloc[-8,0])+true_val\n",
    "   \n",
    "    \n",
    "    true_data_total.append(true_val)\n",
    "    pred_data_total.append(pred_val)   \n",
    "    \n",
    "    ##  calculate the mape for new confirmed cases\n",
    "    pred_test= pre_model.predict([x_val,c_val])   \n",
    "    true_test=y_val*(y_max-y_min)+y_min \n",
    "    pred_test=pred_test*(y_max-y_min)+y_min\n",
    "    new_case_mape=np.mean(abs((pred_test-true_test)/true_test))\n",
    "    new_case_mape_total.append(new_case_mape)\n",
    "    \n",
    "    ## calculate the mape for the culmulative confirmed cases    \n",
    "    true_test_raw=np.array(data_dist.iloc[index1+7:-7,0]).reshape(-1,1)+ true_test\n",
    "    pred_test_raw=np.array(data_dist.iloc[index1+7:-7,0]).reshape(-1,1)+pred_test\n",
    "    cumulative_case_mape=np.mean(abs((pred_test_raw-true_test_raw)/true_test_raw))\n",
    "    cumulative_case_mape_total.append(cumulative_case_mape)\n",
    "\n",
    "#save the result \n",
    "result                          = pd.DataFrame(true_data_total,index=target_cols,columns=['true_data'])\n",
    "result['pred_data']             = pred_data_total\n",
    "result['pred_true_rate']        = result['pred_data']/result['true_data']\n",
    "result['new_case_mape']         = new_case_mape_total\n",
    "result['cumulative_case_mape']  = cumulative_case_mape_total\n",
    "result.to_excel('result_verify.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code is used to predict the CCPM for the next seven days in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:13:42.503840Z",
     "start_time": "2020-06-23T06:13:42.496105Z"
    }
   },
   "outputs": [],
   "source": [
    "#the name of target countries\n",
    "target_cols=['Albania','Algeria','Argentina','Armenia','Australia','Azerbaijan','Bangladesh','Belarus','Belgium','Bermuda',\n",
    "             'Bolivia','Brazil','Bulgaria','Canada','Chile','Colombia','Costa Rica','Cuba','Czech Republic','Denmark',\n",
    "             'El Salvador','Estonia','Finland','France','Ghana','Gibraltar','Greece','Honduras','Hungary','India',\n",
    "             'Indonesia','Iran','Iraq','Ireland','Israel','Jamaica','Jordan','Liberia','Luxembourg','Malaysia','Mexico',\n",
    "             'Morocco','Nepal','Netherlands','Nigeria','Pakistan','Paraguay','Peru','Philippines','Poland','Portugal',\n",
    "             'Qatar','Republic of the Congo','Romania','Russia','Rwanda','Saudi Arabia','Senegal','Sierra Leone',\n",
    "             'Singapore','Slovakia','Slovenia','South Africa','Sri Lanka','Switzerland','Thailand','Tunisia','Turkey',\n",
    "             'Ukraine','United Kingdom','United States','Venezuela']\n",
    "\n",
    "## parameter configure\n",
    "seq_length=7\n",
    "next_days=7\n",
    "\n",
    "INPUT_DIM = 2\n",
    "TIME_STEPS = 7\n",
    "\n",
    "\n",
    "pred_total=[]\n",
    "pre14_true=[]\n",
    "pre7_true=[]\n",
    "\n",
    "pred_control_total=[]\n",
    "pred_nocontrol_total=[]\n",
    "pred_true_total=[]\n",
    "\n",
    "\n",
    "for i in range(len(target_cols)):\n",
    "    print(i,target_cols[i])\n",
    "    \n",
    "    # load the data \n",
    "    country_name='./data/target/'+target_cols[i]+'.xlsx'\n",
    "    data_dist =  pd.read_excel(country_name,encoding='gbk').dropna()\n",
    "    data_dist=data_dist.drop(['截止时间'],axis=1)\n",
    "    data_diff= data_dist['confirmed cases per million'].diff().dropna()\n",
    "    data_diff=pd.concat([data_diff,data_dist.iloc[1:,1]],axis=1)\n",
    "    data_diff=np.array(data_diff)\n",
    "    \n",
    "    x,y,c=create_sequences(data_diff, seq_length, next_days)\n",
    "    c=np.ones((c.shape[0],c.shape[1]))-c.reshape(-1,seq_length)\n",
    "    \n",
    "    \n",
    "    pre14_true.append(y[-8,0])\n",
    "    pre7_true.append(y[-1,0])\n",
    "    \n",
    "    #normalization\n",
    "    x_max=x.max()\n",
    "    x_min=x.min()\n",
    "    x=(x-x_min)/(x_max-x_min)\n",
    "    \n",
    "    y_max=y.max()\n",
    "    y_min=y.min()\n",
    "    y=(y-y_min)/(y_max-y_min)   \n",
    "    \n",
    "    #reshape the dimension\n",
    "    x_train=x.reshape(-1,seq_length,1)\n",
    "    c_train=c.reshape(-1,seq_length)\n",
    "    y_train=y.reshape(-1,1)\n",
    "    \n",
    "    # load the pre-train model and fine-tune the model\n",
    "    source_model='./model/source_model_ALerT-COVID.pkl'  \n",
    "    pre_model= load_model(source_model) \n",
    "    for layer in pre_model.layers[:len(pre_model.layers)-2]:\n",
    "        layer.trainable = False    \n",
    "    pre_model.compile(loss='mse', optimizer=Adam(lr=1e-3))   \n",
    "    pre_model.fit([x_train,c_train], y_train,epochs=10, batch_size=2,verbose = 0)\n",
    " \n",
    "    x_test=data_diff[-7:,0].reshape(1,7,1)   \n",
    "    x_test=(x_test-x_min)/(x_max-x_min)\n",
    "    \n",
    "    #simulation about keeping the lockdown measures or lifting the dockdown measures  \n",
    "    c_test1=np.zeros((1,7))\n",
    "    c_test2=np.ones((1,7))        \n",
    "    c_test3=c_train[-1,:].reshape(1,7)                    \n",
    "        \n",
    "    pred_test1 = pre_model.predict([x_test, c_test1])\n",
    "    pred_test2 = pre_model.predict([x_test, c_test2])\n",
    "    pred_test3 = pre_model.predict([x_test, c_test3])\n",
    "    \n",
    "    pred_test1=np.array(pred_test1)*(y_max-y_min)+y_min    \n",
    "    pred_test2=np.array(pred_test2)*(y_max-y_min)+y_min \n",
    "    pred_test3=np.array(pred_test3)*(y_max-y_min)+y_min \n",
    "    \n",
    "    pred_control_total.append(pred_test1[0,0])\n",
    "    pred_nocontrol_total.append(pred_test2[0,0]) \n",
    "    pred_true_total.append(pred_test3[0,0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T06:13:28.353458Z",
     "start_time": "2020-06-23T06:13:28.276167Z"
    }
   },
   "outputs": [],
   "source": [
    "# save result    \n",
    "result                      = pd.DataFrame(pre14_true,index=target_cols,columns=['pre14_data'])\n",
    "result['pre7_data']         = pre7_true\n",
    "result['pred_control']      = pred_control_total\n",
    "result['pred_nocontrol']    = pred_nocontrol_total\n",
    "result['pred_true']         = pred_true_total\n",
    "result['nocontrol_control'] = (result['pred_nocontrol']-result['pred_control'])/result['pred_control']\n",
    "result.to_excel('result_predict.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
